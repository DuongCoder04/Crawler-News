# H·ªá Th·ªëng Crawler Tin T·ª©c T·ª± ƒê·ªông cho CMS X-Wise

## üìã T·ªïng Quan

T√†i li·ªáu n√†y m√¥ t·∫£ chi ti·∫øt h·ªá th·ªëng crawler t·ª± ƒë·ªông thu th·∫≠p tin t·ª©c t·ª´ c√°c trang b√°o ti·∫øng Vi·ªát v√† ƒë·∫©y v√†o CMS X-Wise th√¥ng qua REST API.

---

## üéØ M·ª•c Ti√™u H·ªá Th·ªëng

1. **T·ª± ƒë·ªông thu th·∫≠p** tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn b√°o ti·∫øng Vi·ªát (VnExpress, ZingNews, Tu·ªïi Tr·∫ª, D√¢n Tr√≠, v.v.)
2. **Tr√≠ch xu·∫•t v√† chu·∫©n h√≥a** d·ªØ li·ªáu ph√π h·ª£p v·ªõi c·∫•u tr√∫c database CMS X-Wise
3. **T√≠ch h·ª£p API** ƒë·ªÉ t·∫°o tin t·ª©c m·ªõi, tr√°nh duplicate
4. **Ch·∫°y ƒë·ªãnh k·ª≥** theo l·ªãch c·∫•u h√¨nh
5. **X·ª≠ l√Ω l·ªói** v√† logging chi ti·∫øt

---

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

### 1. C·∫•u Tr√∫c Database CMS X-Wise (Hi·ªán T·∫°i)

#### B·∫£ng `news`
```sql
CREATE TABLE news (
    id UUID PRIMARY KEY,
    title VARCHAR(500),
    content TEXT,
    created_at DATE,
    reaction_count INT DEFAULT 0,
    status TEXT,  -- 'ACTIVE' | 'INACTIVE'
    category_code VARCHAR(100)
);
```

**L∆∞u √Ω**: Crawler s·∫Ω s·ª≠ d·ª•ng schema hi·ªán c√≥, KH√îNG th√™m tr∆∞·ªùng m·ªõi. Th√¥ng tin ngu·ªìn (source_url, source_name) s·∫Ω ƒë∆∞·ª£c:
- L∆∞u trong Redis cache ƒë·ªÉ check duplicate
- Ho·∫∑c embed v√†o cu·ªëi `content` d∆∞·ªõi d·∫°ng HTML comment n·∫øu c·∫ßn trace ngu·ªìn

#### B·∫£ng `attachment`
```sql
CREATE TABLE attachment (
    id UUID PRIMARY KEY,
    url VARCHAR(255),
    object_type VARCHAR(255),  -- 'news'
    object_id VARCHAR(255),    -- news.id
    created_at DATE,
    status VARCHAR(255),
    file_name VARCHAR(255),
    extension VARCHAR(255)
);
```

#### B·∫£ng `category`
```sql
CREATE TABLE category (
    id UUID PRIMARY KEY,
    name VARCHAR(255),
    code VARCHAR(255) UNIQUE NOT NULL,
    description VARCHAR(255),
    parent_code VARCHAR(255),  -- 'NEWS'
    status VARCHAR(255),
    created_at DATE,
    updated_at DATE
);
```

### 2. API Endpoints CMS X-Wise

#### Base URL
```
Development: https://backend-dev-cms-staging.up.railway.app
Production: [TBD]
```

#### Authentication
```http
Authorization: Bearer <JWT_TOKEN>
```

#### Endpoint: T·∫°o Tin T·ª©c M·ªõi
```http
POST /cms/wise/news
Content-Type: application/json
Authorization: Bearer <JWT_TOKEN>

Request Body:
{
    "title": "string (1-1000 chars, required)",
    "content": "string (1-50000 chars, required, HTML format)",
    "status": "ACTIVE | INACTIVE (default: ACTIVE)",
    "category_code": "string (max 100 chars, required)",
    "attachments": ["uuid1", "uuid2"]  // Array of attachment IDs
}

Response Success (200):
{
    "data": {
        "id": "uuid",
        "title": "...",
        "content": "...",
        "status": "ACTIVE",
        "category_code": "...",
        "created_at": "2026-02-10",
        "reaction_count": 0,
        "attachments": [...]
    },
    "meta": {
        "message": "News created successfully"
    }
}

Response Error (400/401/500):
{
    "meta": {
        "message": "Error message",
        "error": "Detailed error"
    }
}
```

#### Endpoint: Upload Attachment
```http
POST /cms/wise/attachment/upload
Content-Type: multipart/form-data
Authorization: Bearer <JWT_TOKEN>

Request Body:
- file: File (image/document)

Response Success (200):
{
    "data": {
        "id": "uuid",
        "url": "https://...",
        "file_name": "image.jpg",
        "extension": "jpg",
        "status": "ACTIVE",
        "created_at": "2026-02-10"
    },
    "meta": {
        "message": "Upload successful"
    }
}
```

#### Endpoint: L·∫•y Danh S√°ch Categories
```http
GET /cms/wise/categories/by-parent/NEWS
Authorization: Bearer <JWT_TOKEN>

Response Success (200):
{
    "data": [
        {
            "id": "uuid",
            "code": "TECH",
            "name": "C√¥ng ngh·ªá",
            "parent_code": "NEWS",
            "status": "ACTIVE"
        },
        ...
    ]
}
```

---

## üîß C√¥ng Ngh·ªá S·ª≠ D·ª•ng

### Stack Ch√≠nh
- **Python 3.9+**: Ng√¥n ng·ªØ ch√≠nh
- **requests / httpx**: HTTP client cho trang tƒ©nh
- **BeautifulSoup4**: Parse HTML
- **Playwright**: X·ª≠ l√Ω trang JavaScript-rendered
- **APScheduler**: Scheduler ch·∫°y ƒë·ªãnh k·ª≥
- **Redis**: Cache v√† queue (optional)
- **python-dotenv**: Qu·∫£n l√Ω environment variables
- **loguru**: Logging n√¢ng cao

### Th∆∞ Vi·ªán B·ªï Sung
```txt
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
playwright>=1.40.0
apscheduler>=3.10.0
redis>=5.0.0
python-dotenv>=1.0.0
loguru>=0.7.0
validators>=0.22.0
```

---

## üìÅ C·∫•u Tr√∫c D·ª± √Ån

```
news-crawler/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # C·∫•u h√¨nh chung
‚îÇ   ‚îî‚îÄ‚îÄ domains/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ vnexpress.json       # Config VnExpress
‚îÇ       ‚îú‚îÄ‚îÄ zingnews.json        # Config ZingNews
‚îÇ       ‚îú‚îÄ‚îÄ tuoitre.json         # Config Tu·ªïi Tr·∫ª
‚îÇ       ‚îî‚îÄ‚îÄ dantri.json          # Config D√¢n Tr√≠
‚îú‚îÄ‚îÄ engine/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_crawler.py          # Base crawler class
‚îÇ   ‚îú‚îÄ‚îÄ static_crawler.py        # Crawler cho trang tƒ©nh
‚îÇ   ‚îú‚îÄ‚îÄ dynamic_crawler.py       # Crawler cho trang JS
‚îÇ   ‚îî‚îÄ‚îÄ crawlers/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ vnexpress.py         # VnExpress crawler
‚îÇ       ‚îú‚îÄ‚îÄ zingnews.py          # ZingNews crawler
‚îÇ       ‚îú‚îÄ‚îÄ tuoitre.py           # Tu·ªïi Tr·∫ª crawler
‚îÇ       ‚îî‚îÄ‚îÄ dantri.py            # D√¢n Tr√≠ crawler
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ api_client.py            # X-Wise API client
‚îÇ   ‚îú‚îÄ‚îÄ content_cleaner.py       # L√†m s·∫°ch n·ªôi dung
‚îÇ   ‚îú‚îÄ‚îÄ url_normalizer.py        # Chu·∫©n h√≥a URL
‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py          # Rate limiting
‚îÇ   ‚îú‚îÄ‚îÄ robots_checker.py        # Ki·ªÉm tra robots.txt
‚îÇ   ‚îî‚îÄ‚îÄ logger.py                # Logging setup
‚îú‚îÄ‚îÄ scheduler/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ job_scheduler.py         # APScheduler setup
‚îÇ   ‚îî‚îÄ‚îÄ tasks.py                 # ƒê·ªãnh nghƒ©a tasks
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cache.py                 # Redis cache
‚îÇ   ‚îî‚îÄ‚îÄ duplicate_checker.py     # Ki·ªÉm tra duplicate
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_crawlers.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api_client.py
‚îú‚îÄ‚îÄ logs/                        # Th∆∞ m·ª•c logs
‚îú‚îÄ‚îÄ .env.example                 # Environment template
‚îú‚îÄ‚îÄ .env                         # Environment variables (gitignore)
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ main.py                      # Entry point
‚îî‚îÄ‚îÄ README.md                    # H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
```

---

## ‚öôÔ∏è C·∫•u H√¨nh

### 1. Environment Variables (.env)

```bash
# X-Wise API Configuration
XWISE_API_BASE_URL=https://backend-dev-cms-staging.up.railway.app
XWISE_JWT_TOKEN=your_jwt_token_here

# Crawler Configuration
CRAWLER_USER_AGENT=XwiseNewsCrawler/1.0 (+https://x-wise.io; contact@x-wise.io)
CRAWLER_TIMEOUT=30
CRAWLER_MAX_RETRIES=3
CRAWLER_RETRY_DELAY=5

# Rate Limiting (requests per minute)
RATE_LIMIT_VNEXPRESS=30
RATE_LIMIT_ZINGNEWS=30
RATE_LIMIT_TUOITRE=30
RATE_LIMIT_DANTRI=30

# Redis Configuration (Optional)
REDIS_HOST=127.0.0.1
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Scheduler Configuration
SCHEDULER_ENABLED=true
SCHEDULER_TIMEZONE=Asia/Ho_Chi_Minh

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/crawler.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days

# Proxy Configuration (Optional)
PROXY_ENABLED=false
PROXY_HTTP=
PROXY_HTTPS=

# Notification (Optional)
SLACK_WEBHOOK_URL=
EMAIL_NOTIFICATION=false
EMAIL_SMTP_HOST=
EMAIL_SMTP_PORT=
EMAIL_FROM=
EMAIL_TO=
```

### 2. Domain Configuration (JSON)

#### config/domains/vnexpress.json
```json
{
    "domain": "vnexpress.net",
    "name": "VnExpress",
    "enabled": true,
    "crawler_type": "static",
    "category_mapping": {
        "thoi-su": "POLITICS",
        "the-gioi": "WORLD",
        "kinh-doanh": "BUSINESS",
        "giai-tri": "ENTERTAINMENT",
        "the-thao": "SPORTS",
        "phap-luat": "LAW",
        "giao-duc": "EDUCATION",
        "suc-khoe": "HEALTH",
        "doi-song": "LIFESTYLE",
        "du-lich": "TRAVEL",
        "khoa-hoc": "SCIENCE",
        "so-hoa": "TECH",
        "xe": "AUTO",
        "y-kien": "OPINION",
        "tam-su": "STORIES"
    },
    "list_page": {
        "url_pattern": "https://vnexpress.net/{category}",
        "selectors": {
            "article_links": "article.item-news h3.title-news a",
            "pagination": "div.pagination a"
        }
    },
    "detail_page": {
        "selectors": {
            "title": "h1.title-detail",
            "summary": "p.description",
            "content": "article.fck_detail",
            "thumbnail": "meta[property='og:image']",
            "published_date": "span.date",
            "category": "ul.breadcrumb li:last-child a",
            "tags": "div.tags a",
            "author": "p.author_mail strong"
        },
        "remove_elements": [
            "div.box_comment",
            "div.box_tinlienquan",
            "div.ads",
            "script",
            "iframe"
        ]
    },
    "rate_limit": {
        "requests_per_minute": 30,
        "delay_between_requests": 2
    },
    "schedule": {
        "cron": "0 */2 * * *",
        "description": "Ch·∫°y m·ªói 2 gi·ªù"
    }
}
```

#### config/domains/zingnews.json
```json
{
    "domain": "zingnews.vn",
    "name": "ZingNews",
    "enabled": true,
    "crawler_type": "dynamic",
    "category_mapping": {
        "thoi-su": "POLITICS",
        "the-gioi": "WORLD",
        "kinh-doanh-tai-chinh": "BUSINESS",
        "giai-tri": "ENTERTAINMENT",
        "the-thao": "SPORTS",
        "phap-luat": "LAW",
        "giao-duc": "EDUCATION",
        "suc-khoe": "HEALTH",
        "doi-song": "LIFESTYLE",
        "du-lich": "TRAVEL",
        "cong-nghe": "TECH",
        "oto-xe-may": "AUTO"
    },
    "list_page": {
        "url_pattern": "https://zingnews.vn/{category}",
        "selectors": {
            "article_links": "article.article-item h2 a",
            "pagination": "div.pagination a"
        }
    },
    "detail_page": {
        "selectors": {
            "title": "h1.the-article-title",
            "summary": "p.the-article-summary",
            "content": "div.the-article-body",
            "thumbnail": "meta[property='og:image']",
            "published_date": "span.the-article-publish",
            "category": "nav.breadcrumb li:last-child a",
            "tags": "div.the-article-tags a"
        },
        "remove_elements": [
            "div.inner-article",
            "div.ads",
            "script",
            "iframe"
        ]
    },
    "rate_limit": {
        "requests_per_minute": 30,
        "delay_between_requests": 2
    },
    "schedule": {
        "cron": "15 */2 * * *",
        "description": "Ch·∫°y m·ªói 2 gi·ªù, l·ªách 15 ph√∫t"
    }
}
```

---

## üíª Code Implementation

### 1. Base Crawler Class

#### engine/base_crawler.py
```python
"""
Base Crawler Class
Cung c·∫•p c√°c ch·ª©c nƒÉng c∆° b·∫£n cho t·∫•t c·∫£ crawler
"""

import time
import requests
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
from loguru import logger
from utils.rate_limiter import RateLimiter
from utils.robots_checker import RobotsChecker
from utils.url_normalizer import URLNormalizer
from utils.content_cleaner import ContentCleaner


class BaseCrawler(ABC):
    """Base class cho t·∫•t c·∫£ crawler"""
    
    def __init__(self, config: Dict, api_client):
        """
        Args:
            config: Domain configuration t·ª´ JSON
            api_client: X-Wise API client instance
        """
        self.config = config
        self.api_client = api_client
        self.domain = config['domain']
        self.name = config['name']
        
        # Initialize utilities
        self.rate_limiter = RateLimiter(
            requests_per_minute=config['rate_limit']['requests_per_minute']
        )
        self.robots_checker = RobotsChecker(self.domain)
        self.url_normalizer = URLNormalizer()
        self.content_cleaner = ContentCleaner()
        
        # Session setup
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self._get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        logger.info(f"Initialized {self.name} crawler")
    
    def _get_user_agent(self) -> str:
        """Get User-Agent t·ª´ env ho·∫∑c default"""
        import os
        return os.getenv('CRAWLER_USER_AGENT', 
                        f'XwiseNewsCrawler/1.0 (+https://x-wise.io; contact@x-wise.io)')
    
    def fetch_page(self, url: str, retries: int = 3) -> Optional[str]:
        """
        Fetch HTML content t·ª´ URL v·ªõi retry logic
        
        Args:
            url: URL c·∫ßn fetch
            retries: S·ªë l·∫ßn retry
            
        Returns:
            HTML content ho·∫∑c None n·∫øu fail
        """
        # Check robots.txt
        if not self.robots_checker.can_fetch(url):
            logger.warning(f"Blocked by robots.txt: {url}")
            return None
        
        # Rate limiting
        self.rate_limiter.wait_if_needed()
        
        for attempt in range(retries):
            try:
                logger.debug(f"Fetching {url} (attempt {attempt + 1}/{retries})")
                
                response = self.session.get(
                    url,
                    timeout=int(os.getenv('CRAWLER_TIMEOUT', 30))
                )
                response.raise_for_status()
                
                logger.success(f"Successfully fetched: {url}")
                return response.text
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    logger.error(f"403 Forbidden: {url}")
                    return None
                elif e.response.status_code == 404:
                    logger.warning(f"404 Not Found: {url}")
                    return None
                else:
                    logger.error(f"HTTP Error {e.response.status_code}: {url}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"Timeout: {url}")
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Request error: {e}")
            
            # Delay before retry
            if attempt < retries - 1:
                delay = int(os.getenv('CRAWLER_RETRY_DELAY', 5))
                logger.info(f"Retrying in {delay} seconds...")
                time.sleep(delay)
        
        logger.error(f"Failed to fetch after {retries} attempts: {url}")
        return None
    
    @abstractmethod
    def extract_article_links(self, html: str, base_url: str) -> List[str]:
        """
        Tr√≠ch xu·∫•t danh s√°ch link b√†i vi·∫øt t·ª´ trang danh s√°ch
        
        Args:
            html: HTML content
            base_url: Base URL ƒë·ªÉ resolve relative links
            
        Returns:
            List of article URLs
        """
        pass
    
    @abstractmethod
    def extract_article_data(self, html: str, url: str) -> Optional[Dict]:
        """
        Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang chi ti·∫øt b√†i vi·∫øt
        
        Args:
            html: HTML content
            url: Article URL
            
        Returns:
            Dictionary ch·ª©a d·ªØ li·ªáu b√†i vi·∫øt ho·∫∑c None
        """
        pass
    
    def crawl_list_page(self, category: str) -> List[str]:
        """
        Crawl trang danh s√°ch ƒë·ªÉ l·∫•y links b√†i vi·∫øt
        
        Args:
            category: Category slug (e.g., 'thoi-su')
            
        Returns:
            List of article URLs
        """
        url_pattern = self.config['list_page']['url_pattern']
        list_url = url_pattern.format(category=category)
        
        logger.info(f"Crawling list page: {list_url}")
        
        html = self.fetch_page(list_url)
        if not html:
            return []
        
        article_links = self.extract_article_links(html, list_url)
        
        # Normalize URLs
        normalized_links = [
            self.url_normalizer.normalize(link) 
            for link in article_links
        ]
        
        logger.info(f"Found {len(normalized_links)} articles in {category}")
        return normalized_links
    
    def crawl_article(self, url: str) -> Optional[Dict]:
        """
        Crawl chi ti·∫øt m·ªôt b√†i vi·∫øt
        
        Args:
            url: Article URL
            
        Returns:
            Article data dictionary ho·∫∑c None
        """
        logger.info(f"Crawling article: {url}")
        
        html = self.fetch_page(url)
        if not html:
            return None
        
        article_data = self.extract_article_data(html, url)
        
        if article_data:
            # Clean content
            article_data['content'] = self.content_cleaner.clean(
                article_data['content']
            )
            
            # Add metadata
            article_data['source_url'] = url
            article_data['source_name'] = self.name
            
            logger.success(f"Successfully extracted: {article_data['title'][:50]}...")
        
        return article_data
    
    def run(self, categories: Optional[List[str]] = None):
        """
        Ch·∫°y crawler cho c√°c categories
        
        Args:
            categories: List of category slugs, None = all categories
        """
        if not self.config.get('enabled', True):
            logger.warning(f"{self.name} crawler is disabled")
            return
        
        logger.info(f"Starting {self.name} crawler")
        
        # Get categories to crawl
        if categories is None:
            categories = list(self.config['category_mapping'].keys())
        
        total_articles = 0
        successful_articles = 0
        
        for category in categories:
            logger.info(f"Processing category: {category}")
            
            # Get article links
            article_links = self.crawl_list_page(category)
            
            for link in article_links:
                # Crawl article
                article_data = self.crawl_article(link)
                
                if article_data:
                    # Map category
                    xwise_category = self.config['category_mapping'].get(category)
                    article_data['category_code'] = xwise_category
                    
                    # Push to X-Wise
                    success = self.api_client.create_news(article_data)
                    
                    if success:
                        successful_articles += 1
                    
                    total_articles += 1
        
        logger.info(
            f"{self.name} crawler finished: "
            f"{successful_articles}/{total_articles} articles pushed successfully"
        )
```

---

### 2. Static Crawler (cho trang HTML tƒ©nh)

#### engine/static_crawler.py
```python
"""
Static Crawler
X·ª≠ l√Ω c√°c trang web render HTML tƒ©nh (server-side)
"""

from typing import Dict, List, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from loguru import logger
from engine.base_crawler import BaseCrawler


class StaticCrawler(BaseCrawler):
    """Crawler cho trang web tƒ©nh"""
    
    def extract_article_links(self, html: str, base_url: str) -> List[str]:
        """Extract article links t·ª´ list page"""
        soup = BeautifulSoup(html, 'lxml')
        
        selector = self.config['list_page']['selectors']['article_links']
        links = []
        
        for element in soup.select(selector):
            href = element.get('href')
            if href:
                # Convert to absolute URL
                absolute_url = urljoin(base_url, href)
                links.append(absolute_url)
        
        return links
    
    def extract_article_data(self, html: str, url: str) -> Optional[Dict]:
        """Extract article data t·ª´ detail page"""
        soup = BeautifulSoup(html, 'lxml')
        selectors = self.config['detail_page']['selectors']
        
        try:
            # Extract title
            title_elem = soup.select_one(selectors['title'])
            if not title_elem:
                logger.warning(f"No title found: {url}")
                return None
            title = title_elem.get_text(strip=True)
            
            # Extract summary
            summary_elem = soup.select_one(selectors.get('summary', ''))
            summary = summary_elem.get_text(strip=True) if summary_elem else ''
            
            # Extract content
            content_elem = soup.select_one(selectors['content'])
            if not content_elem:
                logger.warning(f"No content found: {url}")
                return None
            
            # Remove unwanted elements
            for remove_selector in self.config['detail_page'].get('remove_elements', []):
                for elem in content_elem.select(remove_selector):
                    elem.decompose()
            
            content = str(content_elem)
            
            # Extract thumbnail
            thumbnail = ''
            if 'thumbnail' in selectors:
                thumb_elem = soup.select_one(selectors['thumbnail'])
                if thumb_elem:
                    thumbnail = thumb_elem.get('content') or thumb_elem.get('src', '')
            
            # Extract published date
            published_date = None
            if 'published_date' in selectors:
                date_elem = soup.select_one(selectors['published_date'])
                if date_elem:
                    published_date = date_elem.get_text(strip=True)
            
            # Extract tags
            tags = []
            if 'tags' in selectors:
                tag_elems = soup.select(selectors['tags'])
                tags = [tag.get_text(strip=True) for tag in tag_elems]
            
            # Extract author
            author = ''
            if 'author' in selectors:
                author_elem = soup.select_one(selectors['author'])
                if author_elem:
                    author = author_elem.get_text(strip=True)
            
            return {
                'title': title,
                'summary': summary,
                'content': content,
                'thumbnail': thumbnail,
                'published_date': published_date,
                'tags': tags,
                'author': author
            }
            
        except Exception as e:
            logger.error(f"Error extracting article data from {url}: {e}")
            return None
```

---

*T√†i li·ªáu ti·∫øp t·ª•c ·ªü ph·∫ßn 2...*


### 3. Dynamic Crawler (cho trang JavaScript-rendered)

#### engine/dynamic_crawler.py
```python
"""
Dynamic Crawler
X·ª≠ l√Ω c√°c trang web render b·∫±ng JavaScript (client-side)
S·ª≠ d·ª•ng Playwright ƒë·ªÉ render trang tr∆∞·ªõc khi extract
"""

from typing import Dict, List, Optional
from playwright.sync_api import sync_playwright, Page, Browser
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from loguru import logger
from engine.base_crawler import BaseCrawler
import time


class DynamicCrawler(BaseCrawler):
    """Crawler cho trang web ƒë·ªông (JavaScript-rendered)"""
    
    def __init__(self, config: Dict, api_client):
        super().__init__(config, api_client)
        self.playwright = None
        self.browser = None
    
    def __enter__(self):
        """Context manager entry"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def fetch_page_dynamic(self, url: str, wait_selector: str = None) -> Optional[str]:
        """
        Fetch page v·ªõi Playwright, ƒë·ª£i JavaScript render
        
        Args:
            url: URL c·∫ßn fetch
            wait_selector: CSS selector ƒë·ªÉ ƒë·ª£i load xong
            
        Returns:
            HTML content sau khi render
        """
        if not self.browser:
            logger.error("Browser not initialized. Use context manager.")
            return None
        
        # Check robots.txt
        if not self.robots_checker.can_fetch(url):
            logger.warning(f"Blocked by robots.txt: {url}")
            return None
        
        # Rate limiting
        self.rate_limiter.wait_if_needed()
        
        try:
            page = self.browser.new_page()
            page.set_extra_http_headers({
                'User-Agent': self._get_user_agent()
            })
            
            logger.debug(f"Loading page with Playwright: {url}")
            
            # Navigate to page
            page.goto(url, wait_until='networkidle', timeout=30000)
            
            # Wait for specific selector if provided
            if wait_selector:
                page.wait_for_selector(wait_selector, timeout=10000)
            else:
                # Default wait for body
                page.wait_for_selector('body', timeout=10000)
            
            # Additional wait for dynamic content
            time.sleep(2)
            
            # Get HTML content
            html = page.content()
            
            page.close()
            
            logger.success(f"Successfully fetched with Playwright: {url}")
            return html
            
        except Exception as e:
            logger.error(f"Error fetching page with Playwright: {e}")
            return None
    
    def extract_article_links(self, html: str, base_url: str) -> List[str]:
        """Extract article links t·ª´ list page"""
        soup = BeautifulSoup(html, 'lxml')
        
        selector = self.config['list_page']['selectors']['article_links']
        links = []
        
        for element in soup.select(selector):
            href = element.get('href')
            if href:
                absolute_url = urljoin(base_url, href)
                links.append(absolute_url)
        
        return links
    
    def extract_article_data(self, html: str, url: str) -> Optional[Dict]:
        """Extract article data t·ª´ detail page"""
        soup = BeautifulSoup(html, 'lxml')
        
        # Try to extract from JSON-LD first (schema.org)
        json_ld_data = self._extract_from_json_ld(soup)
        if json_ld_data:
            logger.info("Extracted data from JSON-LD")
            return json_ld_data
        
        # Fallback to HTML selectors
        return self._extract_from_html(soup, url)
    
    def _extract_from_json_ld(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        Extract article data t·ª´ JSON-LD (schema.org/Article)
        
        Args:
            soup: BeautifulSoup object
            
        Returns:
            Article data ho·∫∑c None
        """
        import json
        
        try:
            # Find JSON-LD script tag
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.string)
                    
                    # Check if it's an Article
                    if isinstance(data, dict) and data.get('@type') in ['Article', 'NewsArticle']:
                        return {
                            'title': data.get('headline', ''),
                            'summary': data.get('description', ''),
                            'content': data.get('articleBody', ''),
                            'thumbnail': data.get('image', {}).get('url', '') if isinstance(data.get('image'), dict) else data.get('image', ''),
                            'published_date': data.get('datePublished', ''),
                            'author': data.get('author', {}).get('name', '') if isinstance(data.get('author'), dict) else '',
                            'tags': data.get('keywords', '').split(',') if isinstance(data.get('keywords'), str) else []
                        }
                        
                except json.JSONDecodeError:
                    continue
                    
        except Exception as e:
            logger.debug(f"No valid JSON-LD found: {e}")
        
        return None
    
    def _extract_from_html(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """Extract article data t·ª´ HTML selectors"""
        selectors = self.config['detail_page']['selectors']
        
        try:
            # Extract title
            title_elem = soup.select_one(selectors['title'])
            if not title_elem:
                logger.warning(f"No title found: {url}")
                return None
            title = title_elem.get_text(strip=True)
            
            # Extract summary
            summary_elem = soup.select_one(selectors.get('summary', ''))
            summary = summary_elem.get_text(strip=True) if summary_elem else ''
            
            # Extract content
            content_elem = soup.select_one(selectors['content'])
            if not content_elem:
                logger.warning(f"No content found: {url}")
                return None
            
            # Remove unwanted elements
            for remove_selector in self.config['detail_page'].get('remove_elements', []):
                for elem in content_elem.select(remove_selector):
                    elem.decompose()
            
            content = str(content_elem)
            
            # Extract thumbnail
            thumbnail = ''
            if 'thumbnail' in selectors:
                thumb_elem = soup.select_one(selectors['thumbnail'])
                if thumb_elem:
                    thumbnail = thumb_elem.get('content') or thumb_elem.get('src', '')
            
            # Extract published date
            published_date = None
            if 'published_date' in selectors:
                date_elem = soup.select_one(selectors['published_date'])
                if date_elem:
                    published_date = date_elem.get_text(strip=True)
            
            # Extract tags
            tags = []
            if 'tags' in selectors:
                tag_elems = soup.select(selectors['tags'])
                tags = [tag.get_text(strip=True) for tag in tag_elems]
            
            # Extract author
            author = ''
            if 'author' in selectors:
                author_elem = soup.select_one(selectors['author'])
                if author_elem:
                    author = author_elem.get_text(strip=True)
            
            return {
                'title': title,
                'summary': summary,
                'content': content,
                'thumbnail': thumbnail,
                'published_date': published_date,
                'tags': tags,
                'author': author
            }
            
        except Exception as e:
            logger.error(f"Error extracting article data from {url}: {e}")
            return None
    
    def crawl_list_page(self, category: str) -> List[str]:
        """Override ƒë·ªÉ s·ª≠ d·ª•ng Playwright"""
        url_pattern = self.config['list_page']['url_pattern']
        list_url = url_pattern.format(category=category)
        
        logger.info(f"Crawling list page with Playwright: {list_url}")
        
        # Use Playwright to fetch
        html = self.fetch_page_dynamic(
            list_url,
            wait_selector=self.config['list_page']['selectors']['article_links']
        )
        
        if not html:
            return []
        
        article_links = self.extract_article_links(html, list_url)
        
        # Normalize URLs
        normalized_links = [
            self.url_normalizer.normalize(link) 
            for link in article_links
        ]
        
        logger.info(f"Found {len(normalized_links)} articles in {category}")
        return normalized_links
    
    def crawl_article(self, url: str) -> Optional[Dict]:
        """Override ƒë·ªÉ s·ª≠ d·ª•ng Playwright"""
        logger.info(f"Crawling article with Playwright: {url}")
        
        # Use Playwright to fetch
        html = self.fetch_page_dynamic(
            url,
            wait_selector=self.config['detail_page']['selectors']['content']
        )
        
        if not html:
            return None
        
        article_data = self.extract_article_data(html, url)
        
        if article_data:
            # Clean content
            article_data['content'] = self.content_cleaner.clean(
                article_data['content']
            )
            
            # Add metadata
            article_data['source_url'] = url
            article_data['source_name'] = self.name
            
            logger.success(f"Successfully extracted: {article_data['title'][:50]}...")
        
        return article_data
```

---

### 4. Utility Classes

#### utils/api_client.py
```python
"""
X-Wise API Client
X·ª≠ l√Ω t·∫•t c·∫£ API calls t·ªõi CMS X-Wise
"""

import os
import requests
from typing import Dict, Optional, List
from loguru import logger
import time


class XWiseAPIClient:
    """Client ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi X-Wise CMS API"""
    
    def __init__(self):
        self.base_url = os.getenv('XWISE_API_BASE_URL', 
                                   'https://backend-dev-cms-staging.up.railway.app')
        self.jwt_token = os.getenv('XWISE_JWT_TOKEN')
        
        if not self.jwt_token:
            raise ValueError("XWISE_JWT_TOKEN not found in environment variables")
        
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {self.jwt_token}',
            'Content-Type': 'application/json'
        })
        
        # Cache categories
        self._categories_cache = None
        
        logger.info(f"Initialized X-Wise API Client: {self.base_url}")
    
    def get_categories(self) -> List[Dict]:
        """
        L·∫•y danh s√°ch categories t·ª´ X-Wise
        
        Returns:
            List of category dictionaries
        """
        if self._categories_cache:
            return self._categories_cache
        
        try:
            url = f"{self.base_url}/cms/wise/categories/by-parent/NEWS"
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            self._categories_cache = data.get('data', [])
            
            logger.info(f"Loaded {len(self._categories_cache)} categories")
            return self._categories_cache
            
        except Exception as e:
            logger.error(f"Error fetching categories: {e}")
            return []
    
    def upload_image(self, image_url: str) -> Optional[str]:
        """
        Download image t·ª´ URL v√† upload l√™n X-Wise
        
        Args:
            image_url: URL c·ªßa ·∫£nh c·∫ßn upload
            
        Returns:
            Attachment ID ho·∫∑c None
        """
        try:
            # Download image
            logger.debug(f"Downloading image: {image_url}")
            img_response = requests.get(image_url, timeout=30, stream=True)
            img_response.raise_for_status()
            
            # Get filename from URL
            filename = image_url.split('/')[-1].split('?')[0]
            if not filename:
                filename = 'image.jpg'
            
            # Upload to X-Wise
            url = f"{self.base_url}/cms/wise/attachment/upload"
            files = {
                'file': (filename, img_response.content, img_response.headers.get('content-type', 'image/jpeg'))
            }
            
            # Remove Content-Type header for multipart/form-data
            headers = {'Authorization': f'Bearer {self.jwt_token}'}
            
            upload_response = requests.post(url, files=files, headers=headers, timeout=60)
            upload_response.raise_for_status()
            
            data = upload_response.json()
            attachment_id = data.get('data', {}).get('id')
            
            if attachment_id:
                logger.success(f"Uploaded image: {attachment_id}")
                return attachment_id
            else:
                logger.warning("No attachment ID in response")
                return None
                
        except Exception as e:
            logger.error(f"Error uploading image {image_url}: {e}")
            return None
    
    def create_news(self, article_data: Dict) -> bool:
        """
        T·∫°o tin t·ª©c m·ªõi tr√™n X-Wise
        
        Args:
            article_data: Dictionary ch·ª©a d·ªØ li·ªáu b√†i vi·∫øt
                - title: str (required)
                - content: str (required)
                - summary: str (optional)
                - category_code: str (required)
                - thumbnail: str (optional, URL)
                - source_url: str (optional, for duplicate check)
                - source_name: str (optional, for logging)
                
        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            # Validate required fields
            if not article_data.get('title'):
                logger.error("Missing required field: title")
                return False
            
            if not article_data.get('content'):
                logger.error("Missing required field: content")
                return False
            
            if not article_data.get('category_code'):
                logger.error("Missing required field: category_code")
                return False
            
            # Check duplicate using cache
            from storage.duplicate_checker import DuplicateChecker
            duplicate_checker = DuplicateChecker()
            
            source_url = article_data.get('source_url')
            if source_url and duplicate_checker.is_crawled(source_url):
                logger.info(f"Article already crawled: {source_url}")
                return False
            
            # Upload thumbnail if exists
            attachment_ids = []
            if article_data.get('thumbnail'):
                attachment_id = self.upload_image(article_data['thumbnail'])
                if attachment_id:
                    attachment_ids.append(attachment_id)
            
            # Add source info to content as HTML comment (for traceability)
            content = article_data['content']
            if source_url:
                source_name = article_data.get('source_name', 'Unknown')
                content += f'\n<!-- Source: {source_name} | URL: {source_url} -->'
            
            # Prepare payload
            payload = {
                'title': article_data['title'][:1000],  # Max 1000 chars
                'content': content[:50000],  # Max 50000 chars
                'status': 'ACTIVE',
                'category_code': article_data['category_code'],
                'attachments': attachment_ids
            }
            
            # Create news
            url = f"{self.base_url}/cms/wise/news"
            response = self.session.post(url, json=payload, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            news_id = data.get('data', {}).get('id')
            
            if news_id:
                # Mark as crawled in cache
                if source_url:
                    duplicate_checker.mark_crawled(source_url, news_id)
                
                logger.success(f"Created news: {news_id} - {article_data['title'][:50]}...")
                return True
            else:
                logger.warning("No news ID in response")
                return False
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 400:
                error_data = e.response.json()
                logger.error(f"Validation error: {error_data}")
            elif e.response.status_code == 401:
                logger.error("Authentication failed. Check JWT token.")
            else:
                logger.error(f"HTTP error {e.response.status_code}: {e}")
            return False
            
        except Exception as e:
            logger.error(f"Error creating news: {e}")
            return False
    
    def check_duplicate(self, source_url: str) -> bool:
        """
        Ki·ªÉm tra xem b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i ch∆∞a
        
        Note: S·ª≠ d·ª•ng Redis cache ƒë·ªÉ check duplicate v√¨ database kh√¥ng c√≥ tr∆∞·ªùng source_url
        
        Args:
            source_url: URL ngu·ªìn c·ªßa b√†i vi·∫øt
            
        Returns:
            True n·∫øu ƒë√£ t·ªìn t·∫°i, False n·∫øu ch∆∞a
        """
        from storage.duplicate_checker import DuplicateChecker
        duplicate_checker = DuplicateChecker()
        return duplicate_checker.is_crawled(source_url)
```

#### utils/content_cleaner.py
```python
"""
Content Cleaner
L√†m s·∫°ch v√† chu·∫©n h√≥a n·ªôi dung HTML
"""

from bs4 import BeautifulSoup
import re
from loguru import logger


class ContentCleaner:
    """L√†m s·∫°ch n·ªôi dung HTML"""
    
    def __init__(self):
        # Danh s√°ch c√°c class/id th∆∞·ªùng ch·ª©a qu·∫£ng c√°o
        self.ad_patterns = [
            r'ad[s]?[-_]',
            r'advertisement',
            r'banner',
            r'sponsor',
            r'promo',
            r'commercial'
        ]
        
        # Danh s√°ch c√°c tag kh√¥ng mong mu·ªën
        self.unwanted_tags = [
            'script', 'style', 'iframe', 'noscript',
            'embed', 'object', 'applet'
        ]
    
    def clean(self, html: str) -> str:
        """
        L√†m s·∫°ch HTML content
        
        Args:
            html: Raw HTML content
            
        Returns:
            Cleaned HTML content
        """
        if not html:
            return ''
        
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # Remove unwanted tags
            for tag_name in self.unwanted_tags:
                for tag in soup.find_all(tag_name):
                    tag.decompose()
            
            # Remove elements with ad-related class/id
            for pattern in self.ad_patterns:
                regex = re.compile(pattern, re.IGNORECASE)
                
                # Remove by class
                for tag in soup.find_all(class_=regex):
                    tag.decompose()
                
                # Remove by id
                for tag in soup.find_all(id=regex):
                    tag.decompose()
            
            # Remove empty paragraphs
            for p in soup.find_all('p'):
                if not p.get_text(strip=True):
                    p.decompose()
            
            # Remove comments
            for comment in soup.find_all(string=lambda text: isinstance(text, str) and text.strip().startswith('<!--')):
                comment.extract()
            
            # Get cleaned HTML
            cleaned_html = str(soup)
            
            # Additional text cleaning
            cleaned_html = self._clean_text(cleaned_html)
            
            return cleaned_html
            
        except Exception as e:
            logger.error(f"Error cleaning content: {e}")
            return html
    
    def _clean_text(self, html: str) -> str:
        """L√†m s·∫°ch text trong HTML"""
        # Remove multiple spaces
        html = re.sub(r'\s+', ' ', html)
        
        # Remove spaces before punctuation
        html = re.sub(r'\s+([.,;:!?])', r'\1', html)
        
        # Remove multiple line breaks
        html = re.sub(r'\n\s*\n', '\n\n', html)
        
        return html.strip()
```

#### utils/url_normalizer.py
```python
"""
URL Normalizer
Chu·∫©n h√≥a v√† l√†m s·∫°ch URLs
"""

from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
import re
from loguru import logger


class URLNormalizer:
    """Chu·∫©n h√≥a URLs"""
    
    def __init__(self):
        # Danh s√°ch query parameters c·∫ßn lo·∫°i b·ªè
        self.unwanted_params = [
            'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
            'fbclid', 'gclid', 'msclkid',
            'ref', 'source', 'campaign',
            '_ga', '_gid'
        ]
    
    def normalize(self, url: str) -> str:
        """
        Chu·∫©n h√≥a URL
        
        Args:
            url: Raw URL
            
        Returns:
            Normalized URL
        """
        try:
            # Parse URL
            parsed = urlparse(url)
            
            # Remove unwanted query parameters
            query_params = parse_qs(parsed.query)
            cleaned_params = {
                k: v for k, v in query_params.items()
                if k not in self.unwanted_params
            }
            
            # Rebuild query string
            new_query = urlencode(cleaned_params, doseq=True)
            
            # Remove fragment
            normalized = urlunparse((
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                new_query,
                ''  # Remove fragment
            ))
            
            # Remove trailing slash
            if normalized.endswith('/') and normalized.count('/') > 3:
                normalized = normalized[:-1]
            
            return normalized
            
        except Exception as e:
            logger.error(f"Error normalizing URL {url}: {e}")
            return url
    
    def is_article_url(self, url: str, domain: str) -> bool:
        """
        Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† b√†i vi·∫øt kh√¥ng
        
        Args:
            url: URL c·∫ßn ki·ªÉm tra
            domain: Domain c·ªßa trang b√°o
            
        Returns:
            True n·∫øu l√† b√†i vi·∫øt, False n·∫øu kh√¥ng
        """
        # Danh s√°ch path patterns kh√¥ng ph·∫£i b√†i vi·∫øt
        excluded_patterns = [
            r'/tag/',
            r'/category/',
            r'/search',
            r'/login',
            r'/register',
            r'/comment',
            r'/page/\d+',
            r'/\d{4}/$',  # Ch·ªâ c√≥ nƒÉm
            r'/\d{4}/\d{2}/$',  # Ch·ªâ c√≥ nƒÉm/th√°ng
        ]
        
        parsed = urlparse(url)
        
        # Check domain
        if domain not in parsed.netloc:
            return False
        
        # Check excluded patterns
        for pattern in excluded_patterns:
            if re.search(pattern, parsed.path):
                return False
        
        # Article URL th∆∞·ªùng c√≥ format: /category/article-slug-123456.html
        # ho·∫∑c /article-slug-123456.html
        if re.search(r'-\d+\.html?$', parsed.path):
            return True
        
        # Ho·∫∑c c√≥ ID trong path
        if re.search(r'/\d{6,}', parsed.path):
            return True
        
        return True  # Default: coi nh∆∞ l√† article
```

#### utils/rate_limiter.py
```python
"""
Rate Limiter
Gi·ªõi h·∫°n s·ªë request per minute
"""

import time
from collections import deque
from loguru import logger


class RateLimiter:
    """Rate limiter s·ª≠ d·ª•ng sliding window"""
    
    def __init__(self, requests_per_minute: int = 30):
        """
        Args:
            requests_per_minute: S·ªë request t·ªëi ƒëa m·ªói ph√∫t
        """
        self.requests_per_minute = requests_per_minute
        self.window_size = 60  # seconds
        self.requests = deque()
    
    def wait_if_needed(self):
        """ƒê·ª£i n·∫øu ƒë√£ v∆∞·ª£t qu√° rate limit"""
        now = time.time()
        
        # Remove old requests outside window
        while self.requests and self.requests[0] < now - self.window_size:
            self.requests.popleft()
        
        # Check if we need to wait
        if len(self.requests) >= self.requests_per_minute:
            # Calculate wait time
            oldest_request = self.requests[0]
            wait_time = self.window_size - (now - oldest_request)
            
            if wait_time > 0:
                logger.debug(f"Rate limit reached. Waiting {wait_time:.2f}s...")
                time.sleep(wait_time)
                
                # Clean up again after waiting
                now = time.time()
                while self.requests and self.requests[0] < now - self.window_size:
                    self.requests.popleft()
        
        # Record this request
        self.requests.append(time.time())
```

#### utils/robots_checker.py
```python
"""
Robots.txt Checker
Ki·ªÉm tra robots.txt tr∆∞·ªõc khi crawl
"""

from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin
from loguru import logger
import requests


class RobotsChecker:
    """Ki·ªÉm tra robots.txt"""
    
    def __init__(self, domain: str):
        """
        Args:
            domain: Domain c·∫ßn ki·ªÉm tra (e.g., 'vnexpress.net')
        """
        self.domain = domain
        self.robots_url = f"https://{domain}/robots.txt"
        self.parser = RobotFileParser()
        self.parser.set_url(self.robots_url)
        
        try:
            self.parser.read()
            logger.info(f"Loaded robots.txt for {domain}")
        except Exception as e:
            logger.warning(f"Could not load robots.txt for {domain}: {e}")
    
    def can_fetch(self, url: str, user_agent: str = '*') -> bool:
        """
        Ki·ªÉm tra xem c√≥ ƒë∆∞·ª£c ph√©p crawl URL kh√¥ng
        
        Args:
            url: URL c·∫ßn ki·ªÉm tra
            user_agent: User agent string
            
        Returns:
            True n·∫øu ƒë∆∞·ª£c ph√©p, False n·∫øu b·ªã c·∫•m
        """
        try:
            return self.parser.can_fetch(user_agent, url)
        except Exception as e:
            logger.warning(f"Error checking robots.txt: {e}")
            return True  # Default: allow if error
    
    def get_crawl_delay(self, user_agent: str = '*') -> float:
        """
        L·∫•y crawl delay t·ª´ robots.txt
        
        Args:
            user_agent: User agent string
            
        Returns:
            Crawl delay in seconds
        """
        try:
            delay = self.parser.crawl_delay(user_agent)
            return float(delay) if delay else 0.0
        except Exception:
            return 0.0
```

#### utils/logger.py
```python
"""
Logger Setup
C·∫•u h√¨nh logging cho to√†n b·ªô h·ªá th·ªëng
"""

import os
import sys
from loguru import logger


def setup_logger():
    """Setup logger v·ªõi rotation v√† retention"""
    
    # Remove default handler
    logger.remove()
    
    # Console handler
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level=os.getenv('LOG_LEVEL', 'INFO'),
        colorize=True
    )
    
    # File handler
    log_file = os.getenv('LOG_FILE', 'logs/crawler.log')
    logger.add(
        log_file,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level=os.getenv('LOG_LEVEL', 'INFO'),
        rotation=os.getenv('LOG_ROTATION', '10 MB'),
        retention=os.getenv('LOG_RETENTION', '30 days'),
        compression='zip'
    )
    
    logger.info("Logger initialized")
```

---

*T√†i li·ªáu ti·∫øp t·ª•c ·ªü ph·∫ßn 3...*


### 5. Scheduler

#### scheduler/job_scheduler.py
```python
"""
Job Scheduler
Qu·∫£n l√Ω l·ªãch ch·∫°y crawler ƒë·ªãnh k·ª≥
"""

import os
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from loguru import logger
from scheduler.tasks import crawl_domain


class CrawlerScheduler:
    """Scheduler cho crawler jobs"""
    
    def __init__(self):
        timezone = os.getenv('SCHEDULER_TIMEZONE', 'Asia/Ho_Chi_Minh')
        self.scheduler = BlockingScheduler(timezone=timezone)
        logger.info(f"Initialized scheduler with timezone: {timezone}")
    
    def add_job(self, domain_config: dict):
        """
        Th√™m job cho m·ªôt domain
        
        Args:
            domain_config: Domain configuration dictionary
        """
        if not domain_config.get('enabled', True):
            logger.info(f"Skipping disabled domain: {domain_config['name']}")
            return
        
        schedule = domain_config.get('schedule', {})
        cron_expr = schedule.get('cron')
        
        if not cron_expr:
            logger.warning(f"No schedule defined for {domain_config['name']}")
            return
        
        # Parse cron expression
        # Format: minute hour day month day_of_week
        parts = cron_expr.split()
        if len(parts) != 5:
            logger.error(f"Invalid cron expression: {cron_expr}")
            return
        
        trigger = CronTrigger(
            minute=parts[0],
            hour=parts[1],
            day=parts[2],
            month=parts[3],
            day_of_week=parts[4],
            timezone=self.scheduler.timezone
        )
        
        job_id = f"crawl_{domain_config['domain']}"
        
        self.scheduler.add_job(
            crawl_domain,
            trigger=trigger,
            args=[domain_config],
            id=job_id,
            name=f"Crawl {domain_config['name']}",
            replace_existing=True
        )
        
        logger.info(
            f"Added job: {domain_config['name']} - "
            f"Schedule: {schedule.get('description', cron_expr)}"
        )
    
    def start(self):
        """Start scheduler"""
        if not os.getenv('SCHEDULER_ENABLED', 'true').lower() == 'true':
            logger.warning("Scheduler is disabled")
            return
        
        logger.info("Starting scheduler...")
        logger.info(f"Jobs: {len(self.scheduler.get_jobs())}")
        
        try:
            self.scheduler.start()
        except (KeyboardInterrupt, SystemExit):
            logger.info("Scheduler stopped")
    
    def list_jobs(self):
        """List all scheduled jobs"""
        jobs = self.scheduler.get_jobs()
        logger.info(f"Scheduled jobs ({len(jobs)}):")
        for job in jobs:
            logger.info(f"  - {job.name} (ID: {job.id}) - Next run: {job.next_run_time}")
```

#### scheduler/tasks.py
```python
"""
Scheduler Tasks
ƒê·ªãnh nghƒ©a c√°c task ƒë∆∞·ª£c scheduler g·ªçi
"""

from loguru import logger
from utils.api_client import XWiseAPIClient
from engine.static_crawler import StaticCrawler
from engine.dynamic_crawler import DynamicCrawler


def crawl_domain(domain_config: dict):
    """
    Task crawl m·ªôt domain
    
    Args:
        domain_config: Domain configuration dictionary
    """
    logger.info(f"Starting scheduled crawl for {domain_config['name']}")
    
    try:
        # Initialize API client
        api_client = XWiseAPIClient()
        
        # Choose crawler type
        crawler_type = domain_config.get('crawler_type', 'static')
        
        if crawler_type == 'dynamic':
            # Use context manager for Playwright
            with DynamicCrawler(domain_config, api_client) as crawler:
                crawler.run()
        else:
            # Static crawler
            crawler = StaticCrawler(domain_config, api_client)
            crawler.run()
        
        logger.success(f"Completed scheduled crawl for {domain_config['name']}")
        
    except Exception as e:
        logger.error(f"Error in scheduled crawl for {domain_config['name']}: {e}")
        # TODO: Send notification (email/slack)
```

---

### 6. Storage & Caching

#### storage/cache.py
```python
"""
Redis Cache
Cache ƒë·ªÉ tr√°nh crawl duplicate v√† l∆∞u tr·ªØ t·∫°m
"""

import os
import redis
from typing import Optional
from loguru import logger
import json


class RedisCache:
    """Redis cache wrapper"""
    
    def __init__(self):
        redis_url = os.getenv('REDIS_URL')
        
        if redis_url:
            self.client = redis.from_url(redis_url, decode_responses=True)
        else:
            self.client = redis.Redis(
                host=os.getenv('REDIS_HOST', '127.0.0.1'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                password=os.getenv('REDIS_PASSWORD'),
                decode_responses=True
            )
        
        try:
            self.client.ping()
            logger.info("Connected to Redis")
        except Exception as e:
            logger.warning(f"Could not connect to Redis: {e}")
            self.client = None
    
    def set(self, key: str, value: str, expire: int = None):
        """Set cache value"""
        if not self.client:
            return
        
        try:
            self.client.set(key, value, ex=expire)
        except Exception as e:
            logger.error(f"Redis set error: {e}")
    
    def get(self, key: str) -> Optional[str]:
        """Get cache value"""
        if not self.client:
            return None
        
        try:
            return self.client.get(key)
        except Exception as e:
            logger.error(f"Redis get error: {e}")
            return None
    
    def exists(self, key: str) -> bool:
        """Check if key exists"""
        if not self.client:
            return False
        
        try:
            return self.client.exists(key) > 0
        except Exception as e:
            logger.error(f"Redis exists error: {e}")
            return False
    
    def delete(self, key: str):
        """Delete key"""
        if not self.client:
            return
        
        try:
            self.client.delete(key)
        except Exception as e:
            logger.error(f"Redis delete error: {e}")
    
    def set_json(self, key: str, value: dict, expire: int = None):
        """Set JSON value"""
        self.set(key, json.dumps(value), expire)
    
    def get_json(self, key: str) -> Optional[dict]:
        """Get JSON value"""
        value = self.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return None
        return None
```

#### storage/duplicate_checker.py
```python
"""
Duplicate Checker
Ki·ªÉm tra b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c crawl ch∆∞a
"""

from storage.cache import RedisCache
from loguru import logger
import hashlib


class DuplicateChecker:
    """Ki·ªÉm tra duplicate articles"""
    
    def __init__(self):
        self.cache = RedisCache()
        self.prefix = "crawler:article:"
        self.ttl = 30 * 24 * 60 * 60  # 30 days
    
    def _get_key(self, url: str) -> str:
        """Generate cache key t·ª´ URL"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return f"{self.prefix}{url_hash}"
    
    def is_crawled(self, url: str) -> bool:
        """
        Ki·ªÉm tra URL ƒë√£ ƒë∆∞·ª£c crawl ch∆∞a
        
        Args:
            url: Article URL
            
        Returns:
            True n·∫øu ƒë√£ crawl, False n·∫øu ch∆∞a
        """
        key = self._get_key(url)
        return self.cache.exists(key)
    
    def mark_crawled(self, url: str, article_id: str = None):
        """
        ƒê√°nh d·∫•u URL ƒë√£ ƒë∆∞·ª£c crawl
        
        Args:
            url: Article URL
            article_id: X-Wise news ID (optional)
        """
        key = self._get_key(url)
        value = article_id or "crawled"
        self.cache.set(key, value, expire=self.ttl)
        logger.debug(f"Marked as crawled: {url}")
    
    def get_article_id(self, url: str) -> str:
        """
        L·∫•y article ID t·ª´ cache
        
        Args:
            url: Article URL
            
        Returns:
            Article ID ho·∫∑c None
        """
        key = self._get_key(url)
        return self.cache.get(key)
```

---

### 7. Domain-Specific Crawlers

#### engine/crawlers/vnexpress.py
```python
"""
VnExpress Crawler
Crawler chuy√™n bi·ªát cho VnExpress.net
"""

from engine.static_crawler import StaticCrawler
from loguru import logger


class VnExpressCrawler(StaticCrawler):
    """Crawler cho VnExpress.net"""
    
    def __init__(self, api_client):
        # Load config
        import json
        with open('config/domains/vnexpress.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        super().__init__(config, api_client)
        logger.info("Initialized VnExpress crawler")
    
    # C√≥ th·ªÉ override c√°c method n·∫øu c·∫ßn custom logic
    # V√≠ d·ª•: x·ª≠ l√Ω ƒë·∫∑c bi·ªát cho VnExpress
```

#### engine/crawlers/zingnews.py
```python
"""
ZingNews Crawler
Crawler chuy√™n bi·ªát cho ZingNews.vn
"""

from engine.dynamic_crawler import DynamicCrawler
from loguru import logger


class ZingNewsCrawler(DynamicCrawler):
    """Crawler cho ZingNews.vn"""
    
    def __init__(self, api_client):
        # Load config
        import json
        with open('config/domains/zingnews.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        super().__init__(config, api_client)
        logger.info("Initialized ZingNews crawler")
    
    # C√≥ th·ªÉ override c√°c method n·∫øu c·∫ßn custom logic
```

---

### 8. Main Entry Point

#### main.py
```python
"""
Main Entry Point
Kh·ªüi ƒë·ªông crawler system
"""

import os
import sys
import json
from pathlib import Path
from dotenv import load_dotenv
from loguru import logger

# Load environment variables
load_dotenv()

# Setup logger
from utils.logger import setup_logger
setup_logger()

from scheduler.job_scheduler import CrawlerScheduler
from utils.api_client import XWiseAPIClient
from engine.static_crawler import StaticCrawler
from engine.dynamic_crawler import DynamicCrawler


def load_domain_configs():
    """Load t·∫•t c·∫£ domain configurations"""
    config_dir = Path('config/domains')
    configs = []
    
    for config_file in config_dir.glob('*.json'):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                configs.append(config)
                logger.info(f"Loaded config: {config['name']}")
        except Exception as e:
            logger.error(f"Error loading {config_file}: {e}")
    
    return configs


def run_once(domain_name: str = None):
    """
    Ch·∫°y crawler m·ªôt l·∫ßn (kh√¥ng d√πng scheduler)
    
    Args:
        domain_name: T√™n domain c·∫ßn crawl, None = all domains
    """
    logger.info("Running crawler in one-time mode")
    
    # Load configs
    configs = load_domain_configs()
    
    # Filter by domain name if specified
    if domain_name:
        configs = [c for c in configs if c['domain'] == domain_name or c['name'] == domain_name]
        if not configs:
            logger.error(f"Domain not found: {domain_name}")
            return
    
    # Initialize API client
    api_client = XWiseAPIClient()
    
    # Run crawlers
    for config in configs:
        if not config.get('enabled', True):
            logger.info(f"Skipping disabled domain: {config['name']}")
            continue
        
        logger.info(f"Crawling {config['name']}...")
        
        try:
            crawler_type = config.get('crawler_type', 'static')
            
            if crawler_type == 'dynamic':
                with DynamicCrawler(config, api_client) as crawler:
                    crawler.run()
            else:
                crawler = StaticCrawler(config, api_client)
                crawler.run()
                
        except Exception as e:
            logger.error(f"Error crawling {config['name']}: {e}")


def run_scheduler():
    """Ch·∫°y crawler v·ªõi scheduler"""
    logger.info("Running crawler in scheduler mode")
    
    # Load configs
    configs = load_domain_configs()
    
    # Initialize scheduler
    scheduler = CrawlerScheduler()
    
    # Add jobs
    for config in configs:
        scheduler.add_job(config)
    
    # List jobs
    scheduler.list_jobs()
    
    # Start scheduler
    scheduler.start()


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='X-Wise News Crawler')
    parser.add_argument(
        '--mode',
        choices=['once', 'scheduler'],
        default='scheduler',
        help='Run mode: once (one-time) or scheduler (continuous)'
    )
    parser.add_argument(
        '--domain',
        type=str,
        help='Domain to crawl (only for once mode)'
    )
    
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("X-Wise News Crawler System")
    logger.info("=" * 60)
    
    try:
        if args.mode == 'once':
            run_once(args.domain)
        else:
            run_scheduler()
            
    except KeyboardInterrupt:
        logger.info("Crawler stopped by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
```

---

## üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng

### 1. C√†i ƒê·∫∑t

```bash
# Clone repository
git clone <repository-url>
cd news-crawler

# T·∫°o virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate  # Windows

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt

# C√†i ƒë·∫∑t Playwright browsers (n·∫øu c·∫ßn crawl trang JS)
playwright install chromium
```

### 2. C·∫•u H√¨nh

```bash
# Copy .env.example sang .env
cp .env.example .env

# Ch·ªânh s·ª≠a .env v·ªõi th√¥ng tin c·ªßa b·∫°n
nano .env
```

**Quan tr·ªçng:** C·∫ßn c·∫•u h√¨nh `XWISE_JWT_TOKEN` ƒë·ªÉ k·∫øt n·ªëi v·ªõi API X-Wise.

### 3. Ch·∫°y Crawler

#### Ch·∫°y m·ªôt l·∫ßn (test)
```bash
# Crawl t·∫•t c·∫£ domains
python main.py --mode once

# Crawl m·ªôt domain c·ª• th·ªÉ
python main.py --mode once --domain vnexpress.net
```

#### Ch·∫°y v·ªõi scheduler (production)
```bash
# Ch·∫°y scheduler (ch·∫°y li√™n t·ª•c theo l·ªãch)
python main.py --mode scheduler

# Ho·∫∑c d√πng nohup ƒë·ªÉ ch·∫°y background
nohup python main.py --mode scheduler > crawler.log 2>&1 &
```

### 4. Ki·ªÉm Tra Logs

```bash
# Xem logs realtime
tail -f logs/crawler.log

# T√¨m l·ªói
grep ERROR logs/crawler.log

# Xem th·ªëng k√™
grep "Successfully" logs/crawler.log | wc -l
```

---

## üìä Monitoring & Alerting

### 1. Log Monitoring

Logs ƒë∆∞·ª£c l∆∞u t·∫°i `logs/crawler.log` v·ªõi c√°c level:
- **DEBUG**: Chi ti·∫øt request/response
- **INFO**: Th√¥ng tin chung
- **SUCCESS**: Th√†nh c√¥ng
- **WARNING**: C·∫£nh b√°o
- **ERROR**: L·ªói

### 2. Notification Setup (Optional)

#### Slack Webhook
```python
# Th√™m v√†o .env
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Trong code (utils/notifier.py)
import requests

def send_slack_alert(message: str):
    webhook_url = os.getenv('SLACK_WEBHOOK_URL')
    if webhook_url:
        requests.post(webhook_url, json={'text': message})
```

#### Email Alert
```python
# Th√™m v√†o .env
EMAIL_NOTIFICATION=true
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_FROM=your-email@gmail.com
EMAIL_TO=admin@x-wise.io

# S·ª≠ d·ª•ng trong error handler
```

---

## üîí B·∫£o M·∫≠t & Best Practices

### 1. Environment Variables
- **KH√îNG BAO GI·ªú** commit file `.env` v√†o git
- S·ª≠ d·ª•ng `.env.example` l√†m template
- Rotate JWT token ƒë·ªãnh k·ª≥

### 2. Rate Limiting
- Tu√¢n th·ªß `robots.txt`
- Kh√¥ng v∆∞·ª£t qu√° rate limit c·ªßa t·ª´ng domain
- S·ª≠ d·ª•ng delay gi·ªØa c√°c request

### 3. Error Handling
- Retry v·ªõi exponential backoff
- Log chi ti·∫øt l·ªói
- Graceful degradation

### 4. Resource Management
- Close connections properly
- Use context managers cho Playwright
- Limit concurrent requests

---

## üß™ Testing

### Unit Tests
```bash
# Ch·∫°y tests
pytest tests/

# V·ªõi coverage
pytest --cov=engine --cov=utils tests/
```

### Test M·ªôt Domain
```python
# test_vnexpress.py
from engine.crawlers.vnexpress import VnExpressCrawler
from utils.api_client import XWiseAPIClient

def test_vnexpress_crawler():
    api_client = XWiseAPIClient()
    crawler = VnExpressCrawler(api_client)
    
    # Test crawl m·ªôt b√†i
    article_data = crawler.crawl_article('https://vnexpress.net/...')
    assert article_data is not None
    assert 'title' in article_data
    assert 'content' in article_data
```

---

## üìà Scaling & Optimization

### 1. Distributed Crawling
```python
# S·ª≠ d·ª•ng Celery cho distributed tasks
from celery import Celery

app = Celery('crawler', broker='redis://localhost:6379/0')

@app.task
def crawl_article_task(url, config):
    # Crawl logic
    pass
```

### 2. Database Storage
```python
# Thay v√¨ ch·ªâ push API, c√≥ th·ªÉ l∆∞u v√†o DB tr∆∞·ªõc
# Sau ƒë√≥ c√≥ worker ri√™ng push l√™n X-Wise
# => TƒÉng reliability, c√≥ th·ªÉ retry
```

### 3. Proxy Rotation
```python
# S·ª≠ d·ª•ng proxy pool ƒë·ªÉ tr√°nh b·ªã block
PROXIES = [
    'http://proxy1:port',
    'http://proxy2:port',
]

# Rotate trong session
```

---

## üêõ Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p

#### 1. 403 Forbidden
```
Nguy√™n nh√¢n: Website block crawler
Gi·∫£i ph√°p:
- Ki·ªÉm tra User-Agent
- S·ª≠ d·ª•ng proxy
- Gi·∫£m rate limit
- Th√™m delay gi·ªØa requests
```

#### 2. Selector kh√¥ng t√¨m th·∫•y element
```
Nguy√™n nh√¢n: Website thay ƒë·ªïi layout
Gi·∫£i ph√°p:
- Ki·ªÉm tra l·∫°i selector trong config
- S·ª≠ d·ª•ng browser DevTools ƒë·ªÉ t√¨m selector m·ªõi
- Update config file
```

#### 3. JWT Token expired
```
Nguy√™n nh√¢n: Token h·∫øt h·∫°n
Gi·∫£i ph√°p:
- Login l·∫°i v√†o CMS X-Wise
- L·∫•y token m·ªõi
- Update .env
```

#### 4. Playwright timeout
```
Nguy√™n nh√¢n: Trang load ch·∫≠m
Gi·∫£i ph√°p:
- TƒÉng timeout trong config
- Ki·ªÉm tra network
- S·ª≠ d·ª•ng wait_for_selector ph√π h·ª£p
```

---

## üìù Ghi Ch√∫ Quan Tr·ªçng

### ƒêi·ªÅu Ch·ªânh Theo H·ªá Th·ªëng X-Wise

1. **Database Schema**: Crawler S·ª¨ D·ª§NG SCHEMA HI·ªÜN T·∫†I, kh√¥ng th√™m tr∆∞·ªùng m·ªõi:
   - Th√¥ng tin `source_url`, `source_name` l∆∞u trong **Redis cache** ƒë·ªÉ check duplicate
   - C√≥ th·ªÉ embed source info v√†o cu·ªëi `content` d∆∞·ªõi d·∫°ng HTML comment n·∫øu c·∫ßn trace
   - TTL cache: 90 ng√†y (configurable)

2. **Duplicate Check**: S·ª≠ d·ª•ng Redis cache thay v√¨ database:
   - Key pattern: `crawler:article:<md5_hash_of_url>`
   - Value: `news_id` (UUID t·ª´ X-Wise)
   - Fast lookup, kh√¥ng c·∫ßn query database
   - N·∫øu Redis clear, c√≥ th·ªÉ crawl l·∫°i (acceptable trade-off)

3. **Category Mapping**: C·∫ßn t·∫°o categories trong X-Wise tr∆∞·ªõc:
   ```sql
   INSERT INTO category (code, name, parent_code, status) VALUES
   ('TECH', 'C√¥ng ngh·ªá', 'NEWS', 'ACTIVE'),
   ('BUSINESS', 'Kinh doanh', 'NEWS', 'ACTIVE'),
   ('SPORTS', 'Th·ªÉ thao', 'NEWS', 'ACTIVE'),
   ...
   ```

4. **Authentication**: JWT token c·∫ßn ƒë∆∞·ª£c refresh ƒë·ªãnh k·ª≥ ho·∫∑c s·ª≠ d·ª•ng service account v·ªõi token kh√¥ng expire.

---

## üéØ Roadmap

### Phase 1: MVP (Hi·ªán t·∫°i)
- ‚úÖ Base crawler framework
- ‚úÖ Static & Dynamic crawlers
- ‚úÖ API integration
- ‚úÖ Scheduler
- ‚úÖ Basic error handling

### Phase 2: Enhancement
- ‚¨ú Duplicate detection v·ªõi database
- ‚¨ú Content similarity check
- ‚¨ú Auto category classification (ML)
- ‚¨ú Image optimization
- ‚¨ú Multi-language support

### Phase 3: Scale
- ‚¨ú Distributed crawling v·ªõi Celery
- ‚¨ú Kubernetes deployment
- ‚¨ú Monitoring dashboard
- ‚¨ú Auto-scaling
- ‚¨ú A/B testing cho selectors

---

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, vui l√≤ng:
1. Ki·ªÉm tra logs t·∫°i `logs/crawler.log`
2. Xem ph·∫ßn Troubleshooting
3. Li√™n h·ªá team X-Wise

---

**T√†i li·ªáu ƒë∆∞·ª£c t·∫°o b·ªüi**: Kiro AI Assistant  
**Ng√†y**: 2026-02-10  
**Version**: 1.0.0
